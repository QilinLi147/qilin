<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introducing Memo2496: A Large-Scale Expert-Annotated Dataset for Music Emotion Recognition | Qilin Li </title> <meta name="author" content="Qilin Li"> <meta name="description" content="We present Memo2496, comprising 2,496 expertly annotated tracks, alongside DAMER, a novel dual-view adaptive framework that achieves state-of-the-art performance across multiple benchmarks."> <meta name="keywords" content="affective-computing, machine-learning, pattern-recognition, emotion-recognition"> <link rel="stylesheet" href="/qilin/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/qilin/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/qilin/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/qilin/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/qilin/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qilinli147.github.io/qilin/blog/2025/memo2496-dataset/"> <script src="/qilin/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/qilin/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/qilin/"> <span class="font-weight-bold">Qilin</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/qilin/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/qilin/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/qilin/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/qilin/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/qilin/people/">people </a> </li> <li class="nav-item active"> <a class="nav-link" href="/qilin/_pages/dropdown/">submenus </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introducing Memo2496: A Large-Scale Expert-Annotated Dataset for Music Emotion Recognition</h1> <p class="post-meta"> Created on December 19, 2025 </p> <p class="post-tags"> <a href="/qilin/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/qilin/blog/tag/music-emotion-recognition"> <i class="fa-solid fa-hashtag fa-sm"></i> music-emotion-recognition</a> Â  <a href="/qilin/blog/tag/affective-computing"> <i class="fa-solid fa-hashtag fa-sm"></i> affective-computing</a> Â  <a href="/qilin/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a> Â  <a href="/qilin/blog/tag/dataset"> <i class="fa-solid fa-hashtag fa-sm"></i> dataset</a> Â  Â· Â  <a href="/qilin/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <figure style="text-align: center; margin: 2rem auto;"> <img src="/qilin/assets/img/cover.png" alt="Memo2496 Dataset" style="max-width: 100%; height: auto;"> <figcaption style="font-style: italic; color: #6c757d; margin-top: 0.5rem; font-size: 0.9rem;"> Memo2496 dataset applications: Music Emotion Recognition, Emotion Induction, Music Generation, and Music Therapy, featuring 2,496 tracks annotated by 30 expert annotators. </figcaption> </figure> <hr> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2512.13998" rel="external nofollow noopener" target="_blank">arXiv:2512.13998</a><br> <strong>Dataset</strong>: <a href="https://ieee-dataport.org/documents/memo2496-expert-annotated-dataset-and-dual-view-adaptive-framework-music-emotion" rel="external nofollow noopener" target="_blank">IEEE DataPort</a> | <a href="https://doi.org/10.6084/m9.figshare.25827034" rel="external nofollow noopener" target="_blank">Figshare</a></p> <hr> <p>Music Emotion Recognition (MER) remains a challenging problem in affective computing, hindered by the scarcity of high-quality annotated datasets and the difficulty of capturing consistent emotional content across diverse musical styles. Our latest work addresses these fundamental limitations through two principal contributions: the <strong>Memo2496 dataset</strong> and the <strong>Dual-view Adaptive Music Emotion Recogniser (DAMER)</strong> framework.</p> <h2 id="the-dataset-challenge">The Dataset Challenge</h2> <p>Existing MER datasets suffer from several critical limitations. Crowdsourced annotations, whilst enabling rapid data collection, introduce systematic noise due to limited musicological expertise amongst non-expert annotators. Moreover, current datasets are often small-scale and stylistically restrictedâ€”PMEmo contains only 794 tracks, whilst EMOPIA focuses narrowly on pop piano compositions. This fragmentation compels researchers to either train models on homogeneous corpora that may overfit to specific niches, or aggregate heterogeneous datasets with incompatible annotation schemes.</p> <h2 id="memo2496-expert-annotation-at-scale">Memo2496: Expert Annotation at Scale</h2> <p>The Memo2496 dataset comprises <strong>2,496 instrumental music tracks</strong> with continuous Valence-Arousal labels, annotated by <strong>30 certified music specialists</strong> from South China University of Technology. Unlike crowdsourced approaches, our annotators possess formal training in music theory and proficiency in diverse instruments, encompassing piano, traditional Chinese instruments, violin, and percussion.</p> <p><strong>Key features:</strong></p> <ul> <li>Rigorous calibration protocol with a consistency threshold of 0.25 Euclidean distance in V-A space</li> <li>Covert duplicate tracks for intra-annotator reliability assessment</li> <li>Structured rest intervals to mitigate annotation fatigue and carryover effects</li> <li>Cross-annotation strategy ensuring each track receives annotations from two distinct cohorts</li> </ul> <p>This methodology yields substantially reduced label noise whilst capturing nuanced emotional progressions across harmonic changes, dynamic variations, and musical form.</p> <h2 id="damer-addressing-cross-track-heterogeneity">DAMER: Addressing Cross-Track Heterogeneity</h2> <p>The DAMER framework integrates three synergistic modules to address persistent MER challenges:</p> <h3 id="dual-stream-attention-fusion-dsaf">Dual-Stream Attention Fusion (DSAF)</h3> <p>DSAF enables token-level bidirectional interaction between Mel spectrograms and cochleagrams through multi-head cross-attention mechanisms. This architecture captures complementary acoustic information: Mel spectrograms provide perceptually motivated frequency representations, whilst cochleagrams model the auditory peripheryâ€™s nonlinear frequency analysis through Gammatone filterbanks.</p> <h3 id="progressive-confidence-labelling-pcl">Progressive Confidence Labelling (PCL)</h3> <p>Semi-supervised learning in MER is susceptible to confirmation bias, where erroneous early predictions propagate through iterative training. PCL implements curriculum-based temperature scheduling (Ï„: 1.5 â†’ 0.7) coupled with Jensen-Shannon divergence-based consistency quantification between dual branches. This enables the model to progressively incorporate unlabelled samples as training stabilises, whilst maintaining high pseudo-label reliability (confidence scores consistently above 0.90).</p> <h3 id="style-anchored-memory-learning-saml">Style-Anchored Memory Learning (SAML)</h3> <p>Musical styles exhibit substantial feature drift across tracks, degrading classifier generalisation. SAML maintains a sliding memory queue of normalised fusion features, employing supervised contrastive loss (InfoNCE) to enforce emotion-discriminative invariance whilst suppressing style-specific confounders. Momentum-based queue updates (coefficient: 0.95) ensure stable class representation with balanced coverage (55%:45% class ratio).</p> <h2 id="experimental-validation">Experimental Validation</h2> <p>DAMER achieves state-of-the-art performance across three benchmarks:</p> <table> <thead> <tr> <th>Dataset</th> <th>Dimension</th> <th>Accuracy</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Memo2496</td> <td>Arousal</td> <td><strong>82.95%</strong></td> <td>+3.43%</td> </tr> <tr> <td>Memo2496</td> <td>Valence</td> <td>78.34%</td> <td>Competitive</td> </tr> <tr> <td>1000songs</td> <td>Arousal</td> <td><strong>81.28%</strong></td> <td>+2.25%</td> </tr> <tr> <td>1000songs</td> <td>Valence</td> <td><strong>70.74%</strong></td> <td>+0.69%</td> </tr> <tr> <td>PMEmo</td> <td>Arousal</td> <td><strong>85.98%</strong></td> <td>+0.17%</td> </tr> <tr> <td>PMEmo</td> <td>Valence</td> <td><strong>77.61%</strong></td> <td>+2.19%</td> </tr> </tbody> </table> <p>Systematic ablation studies confirm each moduleâ€™s contribution: DSAF provides robust multimodal representations (+1.06% arousal accuracy), PCL ensures high-quality pseudo-label supervision (+0.89%), and SAML enforces style-invariant consistency (+1.37% over DSAF+PCL baseline).</p> <h2 id="implications-and-future-directions">Implications and Future Directions</h2> <p>The consistent performance gains across datasets with varying annotation methodologies (expert vs. crowdsourced) and musical styles (instrumental vs. pop) validate DAMERâ€™s generalisation capability. T-SNE visualisation reveals clear class separation in the learned embedding space, whilst progressive confidence labelling dynamics demonstrate sustained pseudo-label coverage above 92% with reliability scores stabilising near 0.90.</p> <p>The Memo2496 dataset opens new possibilities for downstream applications including emotion-driven music generation, therapeutic interventions, and personalised music recommendation systems. Our expert annotation protocol establishes a replicable methodology for future dataset construction in affective computing.</p> <hr> <p><strong>Resources:</strong></p> <ul> <li>ðŸ“„ <strong>Preprint</strong>: <a href="https://arxiv.org/abs/2512.13998" rel="external nofollow noopener" target="_blank">arXiv:2512.13998</a> </li> <li> <table> <tbody> <tr> <td>ðŸ“Š <strong>Dataset</strong>: <a href="https://ieee-dataport.org/documents/memo2496-expert-annotated-dataset-and-dual-view-adaptive-framework-music-emotion" rel="external nofollow noopener" target="_blank">IEEE DataPort</a> </td> <td><a href="https://doi.org/10.6084/m9.figshare.25827034" rel="external nofollow noopener" target="_blank">Figshare (DOI: 10.6084/m9.figshare.25827034)</a></td> </tr> </tbody> </table> </li> <li>ðŸ’» <strong>Code</strong>: <a href="https://github.com/QilinLi147/DAMER" rel="external nofollow noopener" target="_blank">GitHub</a> (coming soon)</li> </ul> <p><strong>Citation:</strong> @misc{li2025memo2496expertannotateddatasetdualview, title={Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition}, author={Qilin Li and C. L. Philip Chen and Tong Zhang}, year={2025}, eprint={2512.13998}, archivePrefix={arXiv}, primaryClass={cs.SD}, url={https://arxiv.org/abs/2512.13998}, }</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Qilin Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/qilin/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/qilin/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/qilin/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/qilin/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/qilin/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/qilin/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/qilin/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/qilin/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/qilin/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/qilin/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/qilin/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/qilin/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/qilin/assets/js/search-data.js"></script> <script src="/qilin/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>