<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://qilinli147.github.io/qilin/feed.xml" rel="self" type="application/atom+xml"/><link href="https://qilinli147.github.io/qilin/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-19T11:11:12+00:00</updated><id>https://qilinli147.github.io/qilin/feed.xml</id><title type="html">blank</title><subtitle>Qilin Li&apos;s academic homepage. Research, publications, and blog. </subtitle><entry><title type="html">Introducing Memo2496: A Large-Scale Expert-Annotated Dataset for Music Emotion Recognition</title><link href="https://qilinli147.github.io/qilin/blog/2025/memo2496-dataset/" rel="alternate" type="text/html" title="Introducing Memo2496: A Large-Scale Expert-Annotated Dataset for Music Emotion Recognition"/><published>2025-12-19T00:00:00+00:00</published><updated>2025-12-19T00:00:00+00:00</updated><id>https://qilinli147.github.io/qilin/blog/2025/memo2496-dataset</id><content type="html" xml:base="https://qilinli147.github.io/qilin/blog/2025/memo2496-dataset/"><![CDATA[<figure style="text-align: center; margin: 2rem auto;"> <img src="/qilin/assets/img/cover.png" alt="Memo2496 Dataset" style="max-width: 100%; height: auto;"/> <figcaption style="font-style: italic; color: #6c757d; margin-top: 0.5rem; font-size: 0.9rem;"> Memo2496 dataset applications: Music Emotion Recognition, Emotion Induction, Music Generation, and Music Therapy, featuring 2,496 tracks annotated by 30 expert annotators. </figcaption> </figure> <hr/> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2512.13998">arXiv:2512.13998</a><br/> <strong>Dataset</strong>: <a href="https://ieee-dataport.org/documents/memo2496-expert-annotated-dataset-and-dual-view-adaptive-framework-music-emotion">IEEE DataPort</a> | <a href="https://doi.org/10.6084/m9.figshare.25827034">Figshare</a></p> <hr/> <p>Music Emotion Recognition (MER) remains a challenging problem in affective computing, hindered by the scarcity of high-quality annotated datasets and the difficulty of capturing consistent emotional content across diverse musical styles. Our latest work addresses these fundamental limitations through two principal contributions: the <strong>Memo2496 dataset</strong> and the <strong>Dual-view Adaptive Music Emotion Recogniser (DAMER)</strong> framework.</p> <h2 id="the-dataset-challenge">The Dataset Challenge</h2> <p>Existing MER datasets suffer from several critical limitations. Crowdsourced annotations, whilst enabling rapid data collection, introduce systematic noise due to limited musicological expertise amongst non-expert annotators. Moreover, current datasets are often small-scale and stylistically restrictedâ€”PMEmo contains only 794 tracks, whilst EMOPIA focuses narrowly on pop piano compositions. This fragmentation compels researchers to either train models on homogeneous corpora that may overfit to specific niches, or aggregate heterogeneous datasets with incompatible annotation schemes.</p> <h2 id="memo2496-expert-annotation-at-scale">Memo2496: Expert Annotation at Scale</h2> <p>The Memo2496 dataset comprises <strong>2,496 instrumental music tracks</strong> with continuous Valence-Arousal labels, annotated by <strong>30 certified music specialists</strong> from South China University of Technology. Unlike crowdsourced approaches, our annotators possess formal training in music theory and proficiency in diverse instruments, encompassing piano, traditional Chinese instruments, violin, and percussion.</p> <p><strong>Key features:</strong></p> <ul> <li>Rigorous calibration protocol with a consistency threshold of 0.25 Euclidean distance in V-A space</li> <li>Covert duplicate tracks for intra-annotator reliability assessment</li> <li>Structured rest intervals to mitigate annotation fatigue and carryover effects</li> <li>Cross-annotation strategy ensuring each track receives annotations from two distinct cohorts</li> </ul> <p>This methodology yields substantially reduced label noise whilst capturing nuanced emotional progressions across harmonic changes, dynamic variations, and musical form.</p> <h2 id="damer-addressing-cross-track-heterogeneity">DAMER: Addressing Cross-Track Heterogeneity</h2> <p>The DAMER framework integrates three synergistic modules to address persistent MER challenges:</p> <h3 id="dual-stream-attention-fusion-dsaf">Dual-Stream Attention Fusion (DSAF)</h3> <p>DSAF enables token-level bidirectional interaction between Mel spectrograms and cochleagrams through multi-head cross-attention mechanisms. This architecture captures complementary acoustic information: Mel spectrograms provide perceptually motivated frequency representations, whilst cochleagrams model the auditory peripheryâ€™s nonlinear frequency analysis through Gammatone filterbanks.</p> <h3 id="progressive-confidence-labelling-pcl">Progressive Confidence Labelling (PCL)</h3> <p>Semi-supervised learning in MER is susceptible to confirmation bias, where erroneous early predictions propagate through iterative training. PCL implements curriculum-based temperature scheduling (Ï„: 1.5 â†’ 0.7) coupled with Jensen-Shannon divergence-based consistency quantification between dual branches. This enables the model to progressively incorporate unlabelled samples as training stabilises, whilst maintaining high pseudo-label reliability (confidence scores consistently above 0.90).</p> <h3 id="style-anchored-memory-learning-saml">Style-Anchored Memory Learning (SAML)</h3> <p>Musical styles exhibit substantial feature drift across tracks, degrading classifier generalisation. SAML maintains a sliding memory queue of normalised fusion features, employing supervised contrastive loss (InfoNCE) to enforce emotion-discriminative invariance whilst suppressing style-specific confounders. Momentum-based queue updates (coefficient: 0.95) ensure stable class representation with balanced coverage (55%:45% class ratio).</p> <h2 id="experimental-validation">Experimental Validation</h2> <p>DAMER achieves state-of-the-art performance across three benchmarks:</p> <table> <thead> <tr> <th>Dataset</th> <th>Dimension</th> <th>Accuracy</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Memo2496</td> <td>Arousal</td> <td><strong>82.95%</strong></td> <td>+3.43%</td> </tr> <tr> <td>Memo2496</td> <td>Valence</td> <td>78.34%</td> <td>Competitive</td> </tr> <tr> <td>1000songs</td> <td>Arousal</td> <td><strong>81.28%</strong></td> <td>+2.25%</td> </tr> <tr> <td>1000songs</td> <td>Valence</td> <td><strong>70.74%</strong></td> <td>+0.69%</td> </tr> <tr> <td>PMEmo</td> <td>Arousal</td> <td><strong>85.98%</strong></td> <td>+0.17%</td> </tr> <tr> <td>PMEmo</td> <td>Valence</td> <td><strong>77.61%</strong></td> <td>+2.19%</td> </tr> </tbody> </table> <p>Systematic ablation studies confirm each moduleâ€™s contribution: DSAF provides robust multimodal representations (+1.06% arousal accuracy), PCL ensures high-quality pseudo-label supervision (+0.89%), and SAML enforces style-invariant consistency (+1.37% over DSAF+PCL baseline).</p> <h2 id="implications-and-future-directions">Implications and Future Directions</h2> <p>The consistent performance gains across datasets with varying annotation methodologies (expert vs. crowdsourced) and musical styles (instrumental vs. pop) validate DAMERâ€™s generalisation capability. T-SNE visualisation reveals clear class separation in the learned embedding space, whilst progressive confidence labelling dynamics demonstrate sustained pseudo-label coverage above 92% with reliability scores stabilising near 0.90.</p> <p>The Memo2496 dataset opens new possibilities for downstream applications including emotion-driven music generation, therapeutic interventions, and personalised music recommendation systems. Our expert annotation protocol establishes a replicable methodology for future dataset construction in affective computing.</p> <hr/> <p><strong>Resources:</strong></p> <ul> <li>ðŸ“„ <strong>Preprint</strong>: <a href="https://arxiv.org/abs/2512.13998">arXiv:2512.13998</a></li> <li> <table> <tbody> <tr> <td>ðŸ“Š <strong>Dataset</strong>: <a href="https://ieee-dataport.org/documents/memo2496-expert-annotated-dataset-and-dual-view-adaptive-framework-music-emotion">IEEE DataPort</a></td> <td><a href="https://doi.org/10.6084/m9.figshare.25827034">Figshare (DOI: 10.6084/m9.figshare.25827034)</a></td> </tr> </tbody> </table> </li> <li>ðŸ’» <strong>Code</strong>: <a href="https://github.com/QilinLi147/DAMER">GitHub</a> (coming soon)</li> </ul> <p><strong>Citation:</strong> @misc{li2025memo2496expertannotateddatasetdualview, title={Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition}, author={Qilin Li and C. L. Philip Chen and Tong Zhang}, year={2025}, eprint={2512.13998}, archivePrefix={arXiv}, primaryClass={cs.SD}, url={https://arxiv.org/abs/2512.13998}, }</p>]]></content><author><name></name></author><category term="research"/><category term="music-emotion-recognition"/><category term="affective-computing"/><category term="deep-learning"/><category term="dataset"/><summary type="html"><![CDATA[We present Memo2496, comprising 2,496 expertly annotated tracks, alongside DAMER, a novel dual-view adaptive framework that achieves state-of-the-art performance across multiple benchmarks.]]></summary></entry></feed>